{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119f215d",
   "metadata": {},
   "source": [
    "# LLM Deployment Notebook (Colab-ready)\n",
    "This notebook is self-contained for the Week 3 assignment. It uses open-source LLMs (Llama-style) and FAISS for RAG.\n",
    "**Security note**: enter your ngrok token at runtime when prompted; do not hardcode secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a189b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate bitsandbytes sentence-transformers faiss-cpu flask pyngrok uvicorn[standard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e2833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prompting files (knowledge base, prompts, QA) from local repo if available.\n",
    "# The notebook will try several candidate directories (uploaded to Colab or mounted Drive).\n",
    "import os, glob, re, yaml\n",
    "\n",
    "candidate_dirs = [\n",
    "    '/content/livedrop-JoeyMerhej/docs',\n",
    "    '/content/livedrop/docs',\n",
    "    '/content/docs',\n",
    "    '/content/drive/MyDrive/livedrop-JoeyMerhej/docs',\n",
    "    '/content/drive/MyDrive/livedrop/docs'\n",
    "]\n",
    "\n",
    "KNOWLEDGE_BASE = []\n",
    "PROMPTS = None\n",
    "GROUND_TRUTH_QA = []\n",
    "\n",
    "def parse_knowledge_base_md(text):\n",
    "    # Expect sections starting with '## Document N: Title'\n",
    "    docs = []\n",
    "    pattern = r'^##\\s+Document\\s+\\d+:\\s*(.+)$'\n",
    "    matches = list(re.finditer(pattern, text, flags=re.M))\n",
    "    if not matches:\n",
    "        # fallback: try splitting on '---' blocks and take first line as title\n",
    "        parts = [p.strip() for p in text.split('\\n---\\n') if p.strip()]\n",
    "        for i, p in enumerate(parts):\n",
    "            title_line = p.splitlines()[0][:200]\n",
    "            docs.append({'id': f'doc{i+1}', 'title': title_line, 'content': p})\n",
    "        return docs\n",
    "    for idx, m in enumerate(matches):\n",
    "        title = m.group(1).strip()\n",
    "        start = m.end()\n",
    "        end = matches[idx+1].start() if idx+1 < len(matches) else len(text)\n",
    "        content = text[start:end].strip()\n",
    "        docs.append({'id': f'doc{idx+1}', 'title': title, 'content': content})\n",
    "    return docs\n",
    "\n",
    "\n",
    "def parse_ground_truth_md(text):\n",
    "    # Simple parser: find blocks starting with '### QNN:' and capture question and authoritative answer\n",
    "    items = []\n",
    "    blocks = re.split(r'\\n\\n###\\s+', text)\n",
    "    for b in blocks:\n",
    "        if not b.strip():\n",
    "            continue\n",
    "        # restore leading ### if stripped\n",
    "        if b.startswith('Q') or b.lower().startswith('q'):\n",
    "            lines = b.splitlines()\n",
    "            # first line: Q##: question\n",
    "            first = lines[0]\n",
    "            qparts = first.split(':',1)\n",
    "            question = qparts[1].strip() if len(qparts)>1 else first.strip()\n",
    "            # find authoritative answer\n",
    "            ans = ''\n",
    "            for i, L in enumerate(lines):\n",
    "                if L.strip().lower().startswith('**authoritative answer:**'):\n",
    "                    # collect following lines until blank\n",
    "                    ans_lines = []\n",
    "                    for rest in lines[i+1:]:\n",
    "                        if rest.strip()=='' and len(ans_lines)>0:\n",
    "                            break\n",
    "                        ans_lines.append(rest.strip(' *'))\n",
    "                    ans = ' '.join([a for a in ans_lines if a])\n",
    "                    break\n",
    "            items.append({'q': question, 'a': ans})\n",
    "    return items\n",
    "\n",
    "found = False\n",
    "for base in candidate_dirs:\n",
    "    kb_path = os.path.join(base, 'prompting', 'knowledge-base.md')\n",
    "    prompts_path = os.path.join(base, 'prompting', 'assistant-prompts.yml')\n",
    "    qa_path = os.path.join(base, 'prompting', 'ground-truth-qa.md')\n",
    "    if os.path.exists(kb_path):\n",
    "        print('Loading knowledge base from', kb_path)\n",
    "        with open(kb_path, 'r', encoding='utf-8') as f:\n",
    "            kb_text = f.read()\n",
    "        KNOWLEDGE_BASE = parse_knowledge_base_md(kb_text)\n",
    "        # load prompts if available\n",
    "        if os.path.exists(prompts_path):\n",
    "            try:\n",
    "                with open(prompts_path, 'r', encoding='utf-8') as f:\n",
    "                    PROMPTS = yaml.safe_load(f)\n",
    "                print('Loaded prompts from', prompts_path)\n",
    "            except Exception as e:\n",
    "                print('Failed to load prompts YAML:', e)\n",
    "        # load QA if available\n",
    "        if os.path.exists(qa_path):\n",
    "            try:\n",
    "                with open(qa_path, 'r', encoding='utf-8') as f:\n",
    "                    qa_text = f.read()\n",
    "                GROUND_TRUTH_QA = parse_ground_truth_md(qa_text)\n",
    "                print('Loaded ground-truth QA from', qa_path)\n",
    "            except Exception as e:\n",
    "                print('Failed to parse ground-truth QA:', e)\n",
    "        found = True\n",
    "        break\n",
    "\n",
    "if not found:\n",
    "    print('No prompting files found in candidate_dirs. Falling back to embedded minimal KB and prompts.')\n",
    "    # Minimal embedded KB (keeps previous content)\n",
    "    KNOWLEDGE_BASE = [\n",
    "        {\n",
    "            'id': 'doc1',\n",
    "            'title': 'Shoplite User Registration Process',\n",
    "            'content': 'To create a Shoplite account, users visit the registration page and provide an email, password, and basic profile information...'\n",
    "        },\n",
    "        {\n",
    "            'id': 'doc2',\n",
    "            'title': 'Shoplite Shopping Cart Features',\n",
    "            'content': 'The Shoplite shopping cart allows users to add multiple items from different sellers, apply promotional codes...'\n",
    "        }\n",
    "    ]\n",
    "    PROMPTS = {\n",
    "        'base_retrieval_prompt': {\n",
    "            'role': 'You are a helpful Shoplite customer service assistant.',\n",
    "            'goal': 'Provide accurate answers using only the provided Shoplite documentation.',\n",
    "            'response_format': 'Answer: [Your response]\\nSources: [List titles]'\n",
    "        }\n",
    "    }\n",
    "\n",
    "print('Knowledge base documents:', len(KNOWLEDGE_BASE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ce114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence-transformers for embeddings and build a FAISS index\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "texts = [d['content'] for d in KNOWLEDGE_BASE]\n",
    "embeddings = embed_model.encode(texts, convert_to_numpy=True)\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "def retrieve(query, k=3):\n",
    "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    return [KNOWLEDGE_BASE[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal generation using a small open-source causal LM from Hugging Face\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# NOTE: In Colab, choose a compatible open-source HF model that fits GPU memory.\n",
    "# Suggested options (pick one based on available GPU VRAM):\n",
    "# - 'tiiuae/falcon-7b-instruct' (good 7B instruct model; requires careful memory handling)\n",
    "# - 'meta-llama/Llama-2-7b-chat-hf' (if you have access and it fits)\n",
    "# - smaller causal models (gpt2, distilgpt2, facebook/opt-125m) are safe fallbacks for low-memory runtimes\n",
    "\n",
    "# We'll default to Falcon-7B instruct as a non-OpenAI open-source model.\n",
    "MODEL_NAME = 'tiiuae/falcon-7b-instruct'\n",
    "SMALL_MODEL = 'gpt2'  # small causal fallback if the large model doesn't fit\n",
    "\n",
    "# Use the new BitsAndBytesConfig via the `quantization_config` param to avoid deprecated args.\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map='auto',\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device_map='auto')\n",
    "except Exception as e:\n",
    "    print('Model load failed or OOM; trying a smaller causal model as fallback. Error:', e)\n",
    "    # Fallback: load a small causal LM that fits in minimal memory (gpt2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(SMALL_MODEL)\n",
    "    model = AutoModelForCausalLM.from_pretrained(SMALL_MODEL)\n",
    "    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device_map='auto')\n",
    "\n",
    "def generate_answer(prompt, max_tokens=200):\n",
    "    out = generator(prompt, max_new_tokens=max_tokens, do_sample=False)[0]['generated_text']\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask app exposing /chat, /ping, /health\n",
    "from flask import Flask, request, jsonify\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/health')\n",
    "def health():\n",
    "    return jsonify({'status':'ok'})\n",
    "\n",
    "@app.route('/ping', methods=['POST'])\n",
    "def ping():\n",
    "    data = request.json or {}\n",
    "    q = data.get('question','')\n",
    "    return jsonify({'answer': q, 'sources': []})\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    data = request.json or {}\n",
    "    q = data.get('question','')\n",
    "    if not q:\n",
    "        return jsonify({'error': 'no question provided'}), 400\n",
    "\n",
    "    # Retrieve top-k documents\n",
    "    docs = retrieve(q, k=3)\n",
    "    context = \"\\n\\n\".join([f\"Title: {d['title']}\\n{d['content']}\" for d in docs])\n",
    "\n",
    "    # Build a safe prompt that instructs the model to use only the provided context\n",
    "    prompt = (\n",
    "        \"You are a helpful Shoplite assistant. Answer the question using only the information in the provided documents. \"\n",
    "        \"If the answer is not present, say you don't know.\\n\\n\" +\n",
    "        f\"{context}\\n\\nQuestion: {q}\\n\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    # Generate answer using the loaded open-source model\n",
    "    try:\n",
    "        answer = generate_answer(prompt, max_tokens=200)\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': f'generation failed: {e}'}), 500\n",
    "\n",
    "    return jsonify({'answer': answer, 'sources': [d['title'] for d in docs]})\n",
    "\n",
    "# Note: In Colab, run the app with uvicorn in a background cell, e.g.:\n",
    "# !uvicorn notebook_app:app --host 0.0.0.0 --port 8080 &\n",
    "# Or save this cell as a Python file and run uvicorn against it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc49033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrok tunnel setup - interactive token input (do NOT hardcode tokens)\n",
    "from pyngrok import ngrok\n",
    "print(\"When prompted, paste your ngrok auth token. It will only be kept in memory for this session.\")\n",
    "ngrok_token = input(\"Enter your ngrok token: \")\n",
    "# Set token in the running process only (not saved to disk)\n",
    "ngrok.set_auth_token(ngrok_token)\n",
    "public_url = ngrok.connect(8080).public_url\n",
    "print('ngrok tunnel created at', public_url)\n",
    "\n",
    "# Usage note: copy `public_url` and use it to call the endpoints from your local machine.\n",
    "# When testing is finished, close the tunnel:\n",
    "# ngrok.disconnect(public_url)\n",
    "# ngrok.kill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e34eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end smoke tests: run 5 ground-truth QA checks and print pass/fail\n",
    "import time, re\n",
    "\n",
    "DEFAULT_PROMPT = {\n",
    "    'role': 'You are a helpful Shoplite customer service assistant.',\n",
    "    'goal': 'Provide accurate answers using only the provided Shoplite documentation.',\n",
    "    'response_format': 'Answer: [Your response]\\nSources: [List titles]'\n",
    "}\n",
    "\n",
    "def build_prompt(prompt_cfg, docs, user_query, max_context_chars=2000):\n",
    "    role = prompt_cfg.get('role','')\n",
    "    goal = prompt_cfg.get('goal','')\n",
    "    response_format = prompt_cfg.get('response_format','')\n",
    "    # Build doc context -- truncate long docs\n",
    "    doc_texts = []\n",
    "    for d in docs:\n",
    "        text = f\"Title: {d['title']}\\n{d['content']}\"\n",
    "        if len(text) > max_context_chars:\n",
    "            text = text[:max_context_chars] + \" ... [TRUNCATED]\"\n",
    "        doc_texts.append(text)\n",
    "    context = \"\\n\\n\".join(doc_texts)\n",
    "    prompt = (\n",
    "        f\"{role}\\n\\nGoal: {goal}\\n\\nUse ONLY the following documents to answer. If the answer isn't present, say you don't know.\\n\\n\"\n",
    "        + f\"{context}\\n\\nQuestion: {user_query}\\n\\n{response_format}\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def score_answer(answer, authoritative):\n",
    "    # Simple overlap heuristic: count longer keyword matches from authoritative answer\n",
    "    a = (authoritative or '').lower()\n",
    "    ans = (answer or '').lower()\n",
    "    keywords = [w for w in re.findall(r\"\\w+\", a) if len(w)>3]\n",
    "    if not keywords:\n",
    "        # if no authoritative tokens, pass if answer is non-empty\n",
    "        return len(ans.strip())>20, 0\n",
    "    matches = sum(1 for kw in keywords if kw in ans)\n",
    "    score = matches / max(1, len(keywords))\n",
    "    # pass threshold: at least 33% of keywords or at least 1 match\n",
    "    passed = (matches >= 1) and (score >= 0.33)\n",
    "    return passed, score\n",
    "\n",
    "\n",
    "def run_smoke_tests(n=5, k=3):\n",
    "    prompt_cfg = (PROMPTS or DEFAULT_PROMPT).get('base_retrieval_prompt', PROMPTS or DEFAULT_PROMPT)\n",
    "    tests = GROUND_TRUTH_QA if GROUND_TRUTH_QA else []\n",
    "    if not tests:\n",
    "        print('No ground-truth QA loaded (GROUND_TRUTH_QA empty). Aborting smoke tests.')\n",
    "        return\n",
    "    total = min(n, len(tests))\n",
    "    passed = 0\n",
    "    results = []\n",
    "    for i in range(total):\n",
    "        item = tests[i]\n",
    "        q = item.get('q') or item.get('question') or ''\n",
    "        authoritative = item.get('a','')\n",
    "        docs = retrieve(q, k=k)\n",
    "        prompt = build_prompt(prompt_cfg, docs, q)\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            ans = generate_answer(prompt, max_tokens=150)\n",
    "        except Exception as e:\n",
    "            ans = f'ERROR: generation failed: {e}'\n",
    "        elapsed = (time.time()-t0)*1000\n",
    "        ok, score = score_answer(ans, authoritative)\n",
    "        results.append({'q': q, 'passed': ok, 'score': score, 'latency_ms': int(elapsed), 'answer_snippet': ans[:300], 'expected_snippet': authoritative[:300]})\n",
    "        if ok: passed += 1\n",
    "    # Summary\n",
    "    print(f\"Smoke test results: {passed}/{total} passed\")\n",
    "    for r in results:\n",
    "        print('---')\n",
    "        print('Q:', r['q'])\n",
    "        print('Pass:', r['passed'], 'Score:', round(r['score'],2), 'Latency(ms):', r['latency_ms'])\n",
    "        print('Answer snippet:', r['answer_snippet'])\n",
    "        print('Expected snippet:', r['expected_snippet'])\n",
    "\n",
    "# Run tests when this cell executes\n",
    "run_smoke_tests(n=5, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe4cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333d512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b596a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b2c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92407bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab1a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
